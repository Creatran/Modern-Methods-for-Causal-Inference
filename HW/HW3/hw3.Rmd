---
title: "Homework 3"
subtitle: "WCM Modern Methods for Causal Inference"
date: "Due on Canvas on June 15, 2020 by 3:00pm"
output: 
  wcmtheme::wcm_html: 
    toc: true
    toc_float: true
    number_toc: true
---

Recall from last week's homework our childhood nutrition story with the following data generating process:

\begin{align*}
U_{W1} &\sim Uniform(0,1) \\
U_{W2}& \sim Uniform(0,1) \\
U_A &\sim Uniform(0, 1) \\
U_Y &\sim Uniform(0, 1)\\
W1& \leftarrow \mathrm{I}[U_{W1}< 0.50]\\
W2& \leftarrow \mathrm{I}[U_{W2}< 0.50]\\
A& \leftarrow \mathrm{I}[U_A< expit(-0.5 + W1 - 1.5^*W2)]\\
Y&\leftarrow \mathrm{I}[U_Y < expit(-0.75 + W1 - 2^*W2 + 2.5^*A +A^*W1 )]
\end{align*}


# The simple substitution estimator based on the G-Computation formula

In last week's homework, we used our knowledge of the true distribution of the observed data $\mathrm{P}$ to evaluate the target parameter $\theta$. Specifically, we plugged in the true conditional mean function $\mathrm{E}(Y|A,W)$ and the marginal distribution $\mathrm{P}(W)$ into the G-computation formula.

We usually do not know the true distribution of the observed data $\mathrm{P}$, and we  we do not observe all 100,000 people in our target population. Instead, we only have a finite (small) sample of $n$ i.i.d. observations of $O$. The empirical distribution puts weight $1/n$ on each observation $O_i$. An intuitive estimator of the statistical estimand  is the simple substitution estimator based on the G-Computation formula. Briefly, the algorithm estimates the relevant parts of the observed data distribution and plugs them into the  parameter mapping $\theta$:
 \begin{align*}
\hat{\theta} &= \frac{1}{n}\sum_{i=1}^n  \big[ \hat{\mathrm{E}}(Y_i |  A_i=1, W_i) - \hat{\mathrm{E}}(Y_i |  A_i=0, W_i) \big]
\end{align*}
where $\hat{\mathrm{E}}(Y|A,W)$ denotes an estimate of the conditional mean function $\mathrm{E}(Y|A,W)$, and the sample proportion (i.e. the empirical mean) has been used  to estimate marginal distribution of covariates $\mathrm{P}(W)$.


Similarly to Lab 3, we will use simulations to evaluate the performance of the simple substitution estimator. This time, we will use various parametric regression models are assumed to estimate $\mathrm{E}(Y|A,W)$. Specifically, for $R=500$ iterations, we will sample $n=200$ i.i.d. observations from $\mathrm{P}$, implement four estimators, and save the resulting point estimates.

1. **Set your seed to `343`, the number of iterations `R` to 500, and number of observations `n` to 200.**

2. **Create an $R=500$ by 4 matrix `estimates` to hold the resulting estimates obtained at each iteration.**  The rows will correspond to iterations and the columns to different estimators  of $\mathrm{E}(Y|A, W)$.

3. **Inside a `for` loop from `r` equals 1 to `R` (500), do the following:**
    a. Sample  $n$ i.i.d.  observations of $O=(W_1,W_2,A,Y)$.
    b. Create a data frame `obs` of the resulting observed data.
    c. Copy the data set `obs` into two new data frames `trt` and `control`. Then set `A=1` for all units in `trt` and set `A=0` for all units in the `control`.
        i. **Estimator \#1:**  Use `glm()` to estimate $\mathrm{E}(Y|A,W)$ (the conditional probability of survival, given the intervention and baseline covariates) based on the following parametric regression model: \[
      \text{Regression1: } \mathrm{E}(Y|A,W) = expit(\beta_0 + \beta_1 A) \]
        ii. **Estimator \#2:** Use `glm()` to estimate $\mathrm{E}(Y|A,W)$  based on the following parametric regression model:\[
    \text{Regression2: }   \mathrm{E}(Y|A,W) = expit(\beta_0 + \beta_1 A+ \beta_2 W_1) \]
       iii. **Estimator \#3:** Use `glm()` to estimate $\mathrm{E}(Y|A,W)$  based on the following parametric regression model:\[
     \text{Regression3: }   \mathrm{E}(Y|A,W) = expit(\beta_0 + \beta_1 A+ \beta_2 W_2) \]
       iv. **Estimator \#4:** Use `glm()` to estimate $\mathrm{E}(Y|A,W)$  based on the following parametric regression model:
      \[ \text{Regression4: }  \mathrm{E}(Y|A,W) = expit(\beta_0 + \beta_1 A+ \beta_2 W_1 + \beta_3 W_2 + \beta_4 A^*W_1 + \beta_5 A^*W_2)  \]
  
  d. For **each** estimator of $\mathrm{E}(Y|A,W)$,  use the `predict` function to get the expected (mean) outcome for each unit under the intervention $\hat{\mathrm{E}}(Y_i|A_i=1,W_i)$. Be sure to specify the arguments `newdata=trt` and the `type='response'`.
  
  e. For **each** estimator of $\mathrm{E}(Y|A,W)$, use the `predict` function to get the expected (mean) outcome for each unit under the control $\hat{\mathrm{E}}(Y_i|A_i=0,W_i)$. Be sure to specify the arguments `newdata=control` and the `type='response'`.
  
  f. For **each** estimator of $\mathrm{E}(Y|A,W)$, estimate $\theta$ by substituting the predicted mean outcomes under the treatment $\hat{\mathrm{E}}(Y_i|A_i=1,W_i)$ and control $\hat{\mathrm{E}}(Y_i|A_i=0,W_i)$ into the G-Computation formula. Then use the sample proportion to estimate the marginal distribution of baseline covariates: \[
  \hat{\theta} = \frac{1}{n}\sum_{i=1}^n  \big[ \hat{\mathrm{E}}(Y_i |A_i=1,W_i) - \hat{\mathrm{E}}(Y_i |  A_i=0, W_i) \big] \]
  
  g. Assign the resulting values as a row in matrix `estimates`.
  
*Hints:*

  - Set your simulations to a low number while you a building your `for` loop

 - The following code assigns the 4 resulting estimates (denoted `theta_hat1`, `theta_hat2`, `theta_hat3`, `theta_hat4`) from iteration `r` to row `r`
  
```{r, eval=F}
estimates[r,] <- c(theta_hat1, theta_hat2, theta_hat3, theta_hat4)
```


# Performance of the estimators

4. **What is the average value of each estimator of  $\theta(\mathrm{P})$ across $R=500$ simulations?**

5. **Estimate the bias of each estimator.** 
    \[Bias\big(\hat{\theta} \big) = \mathrm{E}\big[ \hat{\theta} - \theta \big] \]
    
    *Hint:* For each estimator, average the difference between point estimate $\hat{\theta}$ and the truth $\theta$. You may want to copy your line of code from Homework 2 to evaluate $\theta$.
    
6. **Estimate the variance of each estimator.**
  \[Variance\big(\hat{\theta} \big) = \mathrm{E}\left( \bigg(\hat{\theta} - \mathrm{E}[\hat{\theta} ] \bigg)^2\right) \]
  
7. **Estimate the mean squared error of each estimator.** On average, how far is the estimator from the truth? \[
MSE\big(\hat{\theta}\big) = \mathrm{E}\left( \bigg(\hat{\theta} - \theta \bigg)^2\right)  = Bias^2 + Var\]

8. **Briefly comment on the performance of the estimators in this simulation setting. Which estimator has the lowest MSE over the $R=500$ iterations? Are you surprised?**

# Return to HW1: A new variable

Recall the study question from Homework 1 regarding children's weight gain with and without RUTF treatment. Suppose we receive funding to additionally assess if the child is suffering from an infectious disease 1 month into the study. Let $C$ be an indicator, equaling 1 if the child was sick.

## The Structural Causal Model (SCM)

The above study can be translated into the following structural causal model (SCM):

Endogenous nodes: $X = (W_1, W_2, A, C, Y)$

Structural equations $F$:
\begin{align*}
W_1&=f_{W_1}(U_{W_1})\\
W_2&=f_{W_2}(W_1, U_{W_2})\\
A&=f_A(W_1, W_2, U_A)\\
C&=f_{C}(W_1, W_2, A, U_{C})\\
Y&=f_{Y}(W_1,W_2, A, C, U_{Y})
\end{align*}


## A specific data generating process

11. **Generate the observed data $O$ in the following way:**

  a. Draw $W_1$ from a  $Bernoulli(p=0.20)$
  b. Given $W_1$, draw $W_2$ from a $Bernoulli(p=expit(0.5^*W_1))$
  c. Given $W_1,W_2$ draw $A$ from a $Bernoulli(p=expit[W_1^*W_2])$
  d. Given $W_1,W_2,A$ draw $C$ from a  $Bernoulli(p=expit[-A + 0.3*W_1 -W_2])$
  e. Given $W_1,W_2,A,C$, draw $Y$ from a  $Normal(\mu=(4.4^*A + 0.7^*W_1 - 2^*A^*W_2 -2^*A^*C), \sigma^2=0.3^2)$ The random errors are independent.

## Estimation

12. **Use Monte Carlo simulation to evaluate the true value of the following parameter of the observed data distribution :** \[
\phi = E [ \mathrm{E}(Y|A=1, W_1, W_2, C) - \mathrm{E}(Y|A=0, W_1, W_2, C)]
\]

13. **Use Monte Carlo simulation to evaluate the true value of the average treatment effect:** \[
\theta^* = \mathrm{E}^*(Y_1)- \mathrm{E}^*(Y_0)
\]
Remember that $\theta^*$ is the difference in the counterfactual expected weight gain if all children were given RUTF and the counterfactual expected weight gain if all children were given the standard supplement. 

14. **Does $\phi$ equal $\theta^*$? Why or why not?**
