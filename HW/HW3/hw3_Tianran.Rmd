---
title: "hw3_Tianran"
author: "Tianran Zhang"
date: "6/13/2020"
output: 
  html_document:
    code_folding: hide
    theme: readable
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
library(knitr)
library(kableExtra)
library(dplyr)
```

# The simple substitution estimator based on the G-Computation formula 
1. **Set your seed to `343`, the number of iterations `R` to 500, and number of observations `n` to 200.**
```{r}
set.seed(343)
R <- 500
n <- 200
```

2. **Create an $R=500$ by 4 matrix `estimates` to hold the resulting estimates obtained at each iteration.**  

```{r}
estimates <- matrix(NA, nrow = R, ncol = 4)
```

3. **Inside a `for` loop from `r` equals 1 to `R` (500), do the following:** 
```{r}
set.seed(343)
for (r in 1:R){
  Uw1 <- runif(n)
  Uw2 <- runif(n)
  UA <- runif(n)
  UY <- runif(n)
  
  W1 <- ifelse(Uw1 < 0.5, 1, 0)
  W2 <- ifelse(Uw2 < 0.5, 1, 0)
  A <- ifelse(UA < plogis(-0.5 + W1 - 1.5 * W2), 1, 0)
  Y <- ifelse(UY < plogis(-0.75 + W1 - 2 * W2 + 2.5 * A + A * W1), 
              1, 0)
  
  trt <- control <- obs <- data.frame(W1, W2, A, Y)
  trt$A <- 1
  control$A <- 0
  
  # i. Estimator 1
  fit1 <- glm(Y ~ A, family = binomial(link = "logit"), data = obs)
  mean.treat <- predict(fit1, newdata = trt, type = "response")
  mean.control <- predict(fit1, newdata = control, type = "response")
  theta_hat1 <- mean(mean.treat - mean.control)
  
  # i. Estimator 2
  fit1 <- glm(Y ~ A + W1, family = binomial(link = "logit"), obs)
  mean.treat <- predict(fit1, newdata = trt, type = "response")
  mean.control <- predict(fit1, newdata = control, type = "response")
  theta_hat2 <- mean(mean.treat - mean.control)
  
  # i. Estimator 3
  fit1 <- glm(Y ~ A + W2, family = binomial(link = "logit"), obs)
  mean.treat <- predict(fit1, newdata = trt, type = "response")
  mean.control <- predict(fit1, newdata = control, type = "response")
  theta_hat3 <- mean(mean.treat - mean.control)
  
  # i. Estimator 4
  fit1 <- glm(Y ~ A + W1 + W2 + A*W1 + A*W2, 
              family = binomial(link = "logit"), data = obs)
  mean.treat <- predict(fit1, newdata = trt, type = "response")
  mean.control <- predict(fit1, newdata = control, type = "response")
  theta_hat4 <- mean(mean.treat - mean.control)
  
  estimates[r,] <- c(theta_hat1, theta_hat2, theta_hat3, theta_hat4)
}
```


4. **What is the average value of each estimator of  $\theta(\mathrm{P})$ across $R=500$ simulations?**

```{r}
colnames(estimates) <- paste0("estimator #", 1:4)
data.frame(`theta.P` = apply(estimates, 2, mean)) %>%
  kable() %>%
  kable_styling()
```


5. **Estimate the bias of each estimator.** 
\[Bias\big(\hat{\theta} \big) = \mathrm{E}\big[ \hat{\theta} - \theta \big] \]
```{r}
theta <- (plogis(-0.75+1-2*1+2.5*1+1*1) - 
            plogis(-0.75+1-2*1+2.5*0+0*1))*0.5*0.5+
  (plogis(-0.75+1-2*0+2.5*1+1*1) - 
     plogis(-0.75+1-2*0+2.5*0+0*1))*0.5*0.5 +
  (plogis(-0.75+0-2*1+2.5*1+1*0) - 
     plogis(-0.75+0-2*1+2.5*0+0*0))*0.5*0.5+
  (plogis(-0.75+0-2*0+2.5*1+1*0) - 
     plogis(-0.75+0-2*0+2.5*0+0*0))*0.5*0.5
```

```{r}
data.frame(bias = apply(estimates - theta, 2, mean)) %>%
  kable() %>%
  kable_styling()

```

    
6. **Estimate the variance of each estimator.**
  \[Variance\big(\hat{\theta} \big) = \mathrm{E}\left( \bigg(\hat{\theta} - \mathrm{E}[\hat{\theta} ] \bigg)^2\right) \]
  
```{r}
data.frame(variance = apply(estimates, 2, function(x) mean((x - mean(x))^2))) %>%
  kable() %>%
  kable_styling()
```

7. **Estimate the mean squared error of each estimator.** On average, how far is the estimator from the truth? \[
MSE\big(\hat{\theta}\big) = \mathrm{E}\left( \bigg(\hat{\theta} - \theta \bigg)^2\right)  = Bias^2 + Var\]

```{r}
data.frame(MSE = apply(estimates, 2, function(x) mean((x - theta)^2))) %>%
  kable() %>%
  kable_styling()
```

8. **Briefly comment on the performance of the estimators in this simulation setting. Which estimator has the lowest MSE over the $R=500$ iterations? Are you surprised?**    
  The fourth estimator has the lowset MSE over the R = 500 iterations. I am not surprised since the regression model used by the fourth estimator is the closest one to the true distribution of Y given W1, W2, A.   
  

# Return to HW1: A new variable
  
## A specific data generating process

11. **Generate the observed data $O$ in the following way:**

```{r}
gen_dat <- function(){
  W1 <- rbinom(n, 1, 0.2)
  W2 <- rbinom(n, 1, plogis(0.5 * W1))
  A <- rbinom(n, 1, plogis(W1 * W2))
  C <- rbinom(n, 1, plogis(-A + 0.3 * W1 - W2))
  Y <- rnorm(n, mean = 4.4*A + 0.7*W1 - 2*A*W2 -2*A*C, sd = 0.3)

  data.frame(W1, W2, A, C, Y)
}

```

## Estimation

12. **Use Monte Carlo simulation to evaluate the true value of the following parameter of the observed data distribution :** \[
\phi = E [ \mathrm{E}(Y|A=1, W_1, W_2, C) - \mathrm{E}(Y|A=0, W_1, W_2, C)]
\]

  $\hat{\phi}$ is calculated based on the observed data using Monte Carlo simulation. I calculated the difference in expection at each iteration:  $\phi = E[\mathrm{E}(Y|A=1, W_1, W_2, C) - \mathrm{E}(Y|A=0, W_1, W_2, C)]$, and estimated $\phi$ as the mean of all the differnces: $\hat{\phi} = \frac{1}{n}\sum_{i = 1}^n[ E[\mathrm{E_i}(Y|A=1, W_1, W_2, C) - \mathrm{E_i}(Y|A=0, W_1, W_2, C)]]$.  
```{r}
phi <- theta.star <- numeric()
set.seed(343)

for (i in 1:R){
  dat <- gen_dat()
  dat %>%
    group_by(W1, W2, A, C) %>%
    summarise(EY = mean(Y), n = n()) %>%
    group_by(W1, W2, C) %>%
    summarise(phi = diff(EY) * sum(n)/nrow(dat)) %>%
    pull(phi) %>%
    sum()-> phi[i]
  
  Y1 <- sapply(4.4*1 + 0.7*dat$W1 - 2*1*dat$W2 -2*1*dat$C, rnorm, 
               n = 1, sd = 0.3)
  Y0 <- sapply(4.4*0 + 0.7*dat$W1 - 2*0*dat$W2 -2*0*dat$C, rnorm,
              n = 1, sd = 0.3)
  theta.star[i] <- mean(Y1) - mean(Y0)
}

mean(phi)
```

13. **Use Monte Carlo simulation to evaluate the true value of the average treatment effect:** \[
\theta^* = \mathrm{E}^*(Y_1)- \mathrm{E}^*(Y_0)
\]
Remember that $\theta^*$ is the difference in the counterfactual expected weight gain if all children were given RUTF and the counterfactual expected weight gain if all children were given the standard supplement. 

$\hat{\theta^*}$ is calculated using Monte Carlo simulation. We calculated the difference between the expected mean counterfactual outcomes that all people get A = 1 and  the expected mean counterfactual outcomes that all people get A = 0, and estimated $\theta^*$ as the mean of all differencesï¼š  $\hat{\theta^*} = \sum_{i = 1}^n[\mathrm{E_i}^*(Y_1)- \mathrm{E_i}^*(Y_0)]$. 

```{r}
mean(theta.star)
```
The estimated $\phi$ and $\theta^*$ are very closed to each other.   

14. **Does $\phi$ equal $\theta^*$? Why or why not?**     
  The true $\phi$ equals to $\theta^*$ since they are telling the same story:      
  $$\begin{align}
  \theta^* &= \mathrm{E}^*(Y_1)- \mathrm{E}^*(Y_0)\\
  &= E[E(Y|A = 1, W_1, W_2, C)] - E[E(Y|A = 0, W_1, W_2, C)]\\
  &= E[\mathrm{E}(Y|A=1, W_1, W_2, C) - \mathrm{E}(Y|A=0, W_1, W_2, C)]\\
  &= \phi
  \end{align}$$
  
