---
title: "Lab 6: TMLE"
subtitle: "Modern Methods for Causal Inference"
date: "Due July 20, 2020 at 3:00PM on Canvas"
output: 
  wcmtheme::wcm_html: 
    toc: true
    toc_float: true
    number_toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

We are interested in estimating the causal effect of an after school reading program  on reading comprehension for 2nd graders at an elementary school in Manhattan.

 Suppose we have data on the following variables:

- $W_1$: sex (1 for male; 0 for female) 

- $W_2$: time spent reading at home compared to other 2nd graders (scale from 0 to 1; with higher values for more time spent reading at home) 

- $W_3$: English writing skills evaluated at the beginning of 2nd grade (continuous and scaled; with larger, positive values for better writing) 

- $W_4$: Reading comprehension skills evaluated at the beginning of 2nd grade (continuous and scaled; with larger, positive values for better reading comprehension) 

- $A$: whether the student's parents enrolled them in the after school reading program at the beginning of 2nd grade (1 for yes; 0 for no). For simplicity we will assume that all students who were enrolled attended all sessions.

- $Y$: Reading comprehension improvement from the beginning of 2nd grade (scale from 0 to 1; with higher values for more improvement; for simplicity let's assume all students improved from the beginning to the end of 2nd grade) 

Let $W=(W_1,W_2,W_3,W_4)$ be the vector of baseline covariates. 


# Causal Roadmap Rundown

**Step 1: Specify the Question:** What is the causal effect of the after school reading program on reading comprehension in 2nd graders?

**Step 2: Specify the structural causal model (SCM):**

- Endogenous nodes: $X= (W, A, Y)$, where $W=(W_1,W_2,W_3,W_4)$ is the set of baseline covariates (sex, prior reading and writing z-scores, and time spent reading at home), $A$ is whether the child is enrolled in the program, and $Y$ is reading comprehension improvement. For simplicity, we have condensed the baseline characteristics into a single node.

- Background variables (Exogenous nodes): $U = (U_W, U_A, U_Y) \sim \mathrm{P}^*$. We place no assumptions on the distribution $\mathrm{P}^*$. 

- Structural equations:  \begin{align*}
W &\leftarrow f_{W}(U_W) \\
A &\leftarrow f_A(W, U_A) \\
Y &\leftarrow f_Y(W, A, U_Y) \end{align*}
We have  not placed any restrictions on the functional forms.

**Step 3: Specify the causal parameter of interest:** We are interested in the causal effect of the after school reading program on reading comprehension scores in 2nd graders at this Manhattan school (i.e. the average treatment effect): \[
\theta^* = \mathrm{E}^*(Y_1) - \mathrm{E}^*(Y_0)
\] 
where $Y_a$ is the counterfactual outcome (reading comprehension score difference from 1st grade), if possibly contrary to fact, the child had attended the after school program $A=a$.

**Step 4: Specify the link between the structural causal model (SCM) and the observed data:** We assume that the observed data $O=(W,A,Y) \sim \mathrm{P}$ were generated by sampling $n$ times from a data generating compatible with the SCM. The statistical model for the set of allowed distributions of the observed data is non-parametric.

**Step 5: Assess identifiability:** In the original structural causal model, the target causal parameter is not identified from the observed data distribution. For identifiability to hold, we would need the following independence assumptions to hold: $U_A \perp U_Y$ and (i) $U_A \perp U_{W}$, or (ii) $U_Y \perp U_{W}$. We also need the positivity assumption to hold \[
0 < P(A=1|W=w) < 1
\]
for all $w$ for which $\mathrm{P}(W=w) >0$. In terms of our example, there must be a positive probability of the children enrolling in the after-school program  within strata of baseline covariates.

**Step 6: Specify the statistical estimand:** Even though the identifiability assumptions do not hold in the original structural causal model, we can still specify a statistical estimand that would equal the wished-for causal effect if the identifiability assumptions did, in fact, hold.

The target statistical estimand is given by the G-Computation formula:
\begin{align*}
\theta &= \mathrm{E}\big[ \mathrm{E}(Y|A=1, W) - \mathrm{E}(Y|A=0,W) \big] \\
  &= \sum_w \big[ \mathrm{E}(Y|A=1, W=w) -\mathrm{E}(Y|A=0, W=w)  \big]\mathrm{P}(W=w)
\end{align*}

**Step 7: Estimate the chosen parameter of the observed data distribution:**

  (a) **Simple substitution estimator based on the G-Computation formula:**
\[
\hat{\theta}_{G-comp} = \frac{1}{n} \sum_{i=1}^n  \bigg( \hat{\mathrm{E}}(Y_i | A_i=1,W_i) - \hat{\mathrm{E}}(Y_i | A_i=0,W_i) \bigg)
\]
where $\hat{\mathrm{E}}(Y| A,W)$ is the estimator of the conditional mean outcome given the exposure (children attending the after school reading program or not) and baseline covariates $\mathrm{E}(Y|A,W)$. Consistency of the simple (non-targeted) substitution estimator depends on consistent estimation of the conditional mean outcome $\mathrm{E}(Y|A,W)$.

  (b) **Standard (unstabilized) inverse probability weighted estimator (IPTW):**
\[
\hat{\theta}_{IPTW} = \frac{1}{n}\sum_{i=1}^n \left( \frac{\mathrm{I}(A_i=1)}{\hat{\mathrm{P}}(A_i=1|W_i)}  -
   \frac{\mathrm{I}(A_i=0)}{\hat{\mathrm{P}}(A_i=0|W_i)} \right) Y_i
\]
where $\hat{\mathrm{P}}(A=1|W)$ is the estimator of the exposure mechanism (i.e. the conditional probability of attending the program, given the baseline covariates). Consistency of IPTW estimators depends on consistent estimation of the exposure mechanism $\mathrm{P}(A|W)$. 

  (c) **Targeted maximum likelihood estimation (TMLE):**
\[
\hat{\theta}_{TMLE} =  \frac{1}{n} \sum_{i=1}^n  \bigg[ \tilde{m}(A_i,W_i) \bigg]
\]
where $\tilde{m}(A,W)$ denotes the targeted estimate of the conditional mean outcome, given the exposure and baseline covariates $\mathrm{E}(Y|A,W)$.

  - Implementation requires estimation of both the conditional mean function $\mathrm{E}(Y|A,W)$ and the exposure mechanism $g(W)=\mathrm{P}(A=1|W)$. 

  - Double robust estimators are consistent if either $\mathrm{E}(Y|A,W)$ or $\mathrm{P}(A|W)$ are estimated consistently.

  - If both $\mathrm{E}(Y|A,W)$ and $\mathrm{P}(A|W)$ are estimated consistently, TMLE will be efficient (achieve the lowest possible asymptotic variance).

  - These asymptotic properties describe what happens when sample size goes to infinity and also translate into lower bias and variance in finite samples.

  - If we apply an estimator to our observed data ($n$ i.i.d. copies of $O$ drawn from $\mathrm{P}$), we get an estimate (a number). The estimator is function of random variables; so it is a random variable. It has a distribution, which we can study theoretically or using simulations. *Note:* An estimator is *consistent* if the point estimates converge (in probability) to the estimand as sample size $n \rightarrow \infty$.

# Data import

**1. Set the seed to 252, import `RLab6_TMLE.csv`, assign it to dataframe `ObsData`, and assign the number of students in the data set to `n`.**

**2. Explore the data set using the `summary`, `head`, and any other functions to help you understand the data. Only include code; do not evaluate this chunk of code when you turn in your assignment.**

# Implement TMLE for the G-computation estimand

**3. Load the `SuperLearner` package. Then specify the Super Learner library with the following algorithms: `SL.glm`, `SL.step` and `SL.gam`.** In practice, we would want to use a larger library with a mixture of simple (e.g. parametric) and more aggressive libraries.

**4. Use Super Learner to estimate $\mathrm{E}(Y|A, W)$, which is the expected reading comprehension difference given the exposure and baseline covariates.**

  (a) Create dataframe `X` consisting of the covariates ($W_1,W_2,W_3,W_4$) and the exposure $A$.  Also create dataframe `X1` where $A$ has been set to 1, and create dataframe `X0` where $A$ has been set to 0.

  (b) Estimate $m(A,W)=\mathrm{E}(Y|A,W)$ by running `SuperLearner`. Call this object `SL.outcome`.  Be sure to specify the `SL.library` and the appropriate `family`.

  (c) Use the `predict` function to obtain initial estimates of the expected outcome, given the observed exposure and covariates $\hat{m}(A,W)=\hat{\mathrm{E}}(Y|A,W)$. The argument `newdata=ObsData` specifies that we want to predict the outcome using as input the observed exposure and covariates.

  (d) Also obtain the initial estimates of the expected outcome for all units under the exposure $\hat{\mathrm{E}}(Y|A=1,W)$. Now we specify  `newdata=X1` to predict the outcome using as input `X1`, where $A=1$ for all units.

  (e) Finally, obtain the initial estimates of the expected outcome for all units under no exposure $\hat{\mathrm{E}}(Y|A=0,W)$. Now we specify `newdata=X0` to predict the outcome using as input `X0`, where $A=0$ for all units.

  (f)  Evaluate the simple substitution estimator by plugging the estimates $\hat{\mathrm{E}}(Y|A=1,W)$ and $\hat{\mathrm{E}}(Y|A=0,W)$ into the target parameter mapping:
  \[
\hat{\theta}_{G-comp} = \frac{1}{n} \sum_{i=1}^n \bigg( \hat{\mathrm{E}}(Y_i | A_i=1,W_i) - \hat{\mathrm{E}}(Y_i | A_i=0,W_i) \bigg)
\] *Note:* This step is not part of the TMLE algorithm, but done for comparison.

**5. Estimate the exposure mechanism $g(W) = \mathrm{P}(A=1|W)$, which is the conditional probability of attending the program, given baseline covariates.**

  (a) Estimate $\mathrm{P}(A|W)$ by running `SuperLearner`. Call this object `SL_exposure`. Since we are estimating the exposure mechanism, specify the-outcome-for-prediction with `Y=ObsData$A` and the predictors as the baseline covariates with `X=dplyr::select(ObsData, -A, -Y)`. Use the same library.

  (b) Estimate the predicted probability of attending the reading program, given the children's baseline characteristics $\hat{\mathrm{P}}(A=1|W)$. These be accessed with `SL_exposure$SL.predict`. Look at the distribution of estimated probabilities: $g(W) = \hat{\mathrm{P}}(A=1|W)$ and $1-g(W) = \hat{\mathrm{P}}(A=0|W)$.
   
**6. Use these estimates to create the clever covariate:**
\[
\hat{H}(A, W) = \left(\frac{\mathrm{I}(A=1)}{\hat{g}(W)} - \frac{\mathrm{I}(A=0)}{1-\hat{g}(W)}  \right)
\]

  (a) Calculate `H_AW` for each child:

  For children with $A=1$,the clever covariate is 1 over the predicted probability of being in the reading program, given the baseline covariates.  Among children with $A=0$, the clever covariate is -1 over the predicted probability of not being in the reading program, given the baseline covariates.

  (b) Also evaluate the clever covariate at $A=1$ and $A=0$ for all children. Call the resulting vectors `H_1W` and `H_0W`, respectively.

  (c) Evaluate the IPTW estimator by taking the empirical mean of the weighted observations:
  \begin{align*}
  \hat{\theta}_{IPTW} & =
    \frac{1}{n}\sum_{i=1}^n \left[ \frac{\mathrm{I}(A_i=1)}{\hat{g}(W)} - \frac{\mathrm{I}(A_i=0)}{1-\hat{g}(W)} \right] Y_i \\
    &=\frac{1}{n}\sum_{i=1}^n \hat{H}(A_i,W_i)\times Y_i
  \end{align*}
As before, this is not part of the TMLE algorithm, but implemented for comparison.

**7. Target the initial estimator of the conditional mean outcome $\hat{\mathrm{E}}(Y|A,W)$ with information in the estimated exposure mechanism  $\hat{\mathrm{P}}(A=1|W)$.**

  (a) Run a univariate regression of the outcome $Y$ on the clever covariate $\hat{H}(A,W)$ with the (logit of the) initial estimates as offset. Specifically, we estimate the coefficient $\epsilon$ by fitting the following logistic regression model
\begin{align*}
logit[\hat{m}(A,W)] &= logit [\hat{m}(A,W)] + \hat{\epsilon} \hat{H}(A,W) \\
\tilde{m}(A,W) &=  expit \bigg[ logit\big[ \hat{m}(A,W) \big] + \hat{\epsilon} \hat{H}(A,W) \bigg]
\end{align*}
Note there is no intercept (i.e. there is no $\beta_0$ term), and the coefficient on the ($logit$ of the) initial estimator is set to 1.

  - We are again calling the `glm` function to fit a generalized linear model.
  
  - On the left hand side of the formula, we have the outcome $Y$.
  
  - On the right hand side of the formula, we suppress the intercept by including -1  and use as `offset`  the $logit$ of our initial Super Learner estimates `expY.givenAW`.
  
  - In `R`, $logit(x)=log(x/(1-x))$ function is given by `qlogis(x)`.
  
  - The only main term in the regression is the clever covariate $\hat{H}(A,W)$.

  (b) Let `epsilon` denote the resulting maximum likelihood estimate of the coefficient on the clever covariate `H.AW`.

  (c) Update the initial estimate of $\hat{\mathrm{E}}(Y|A,W)$ according to the fluctuation model: 
  
\begin{align*}
logit[\hat{m}(A,W)] &= logit [\hat{m}(A,W)] + \hat{\epsilon} \hat{H}(A,W) \\
\tilde{m}(A,W) &=  expit \bigg[ logit\big[ \hat{m}(A,W) \big] + \hat{\epsilon} \hat{H}(A,W) \bigg]
\end{align*}


  (d)  Plug-in the estimated coefficient $\hat{\epsilon}$ to yield targeted estimates of the expected outcome under the exposure $\hat{\mathrm{E}}^*(Y|A=1,W)$ and under no exposure $\hat{\mathrm{E}}^*(Y|A=0,W)$: 
\begin{align*}
\tilde{m}(1,W) &= expit \bigg[ logit\big[ \hat{m}(1,W) \big] + \hat{\epsilon} \hat{H}(1,W) \bigg]\\
\tilde{m}(0,W) &= expit \bigg[ logit\big[ \hat{m}(0,W) \big] + \hat{\epsilon} \hat{H}(0,W) \bigg]\\
\end{align*}

**8. Estimate the statistical parameter by substituting the targeted predictions into the G-Computation formula.**

**9. Estimate $\theta$ by averaging the difference in the targeted predictions:** 
\[
\hat{\theta}_{TMLE} =  \frac{1}{n} \sum_{i=1}^n  \bigg[ \tilde{m}(A,W) \bigg]
\]

**10. Compare with IPTW and the simple substitution results from earlier.**

# Variance estimation

## Using the influence curve

**11. Calculate the standard error of the TMLE estimate using the following equation:**

```{r, eval=F}
tmle_se <- sd(H_AW *(ObsData$Y - expY_givenAW_star) + expY_given1W_star - expY_given0W_star) / sqrt(n)
tmle_se
```

*Note:* in this `R` code, the `_star` indicates the updated $E[Y|A,W]$ estimates, also written as $\tilde{m}(w)$.

**12. Briefly explain the background for this equation for the standard errors.**

## Implementing the non-parametric bootstrap

Alternatively, you could use the non-parametric bootstrap for variance estimation for statistical inference.

**13. Implement the non-parametric bootstrap with 500 iterations by creating a bootstrapped sample of your data (sampling with replacement) and implementing your (hand-coded) TMLE function. Save the estimates in a resulting vector called `estimates`.**

**14. Look at a histogram of the bootstrapped `estimates` and comment on your findings.**

**15. Assuming a normal distribution, compute a 95% confidence interval.**

**16. Using the `quantiles` function, find the 2.5% and 97.5% quantiles and use them to compute a 95% confidence interval for the point estimates.**

# The basics of the `tmle` package

**17. Load the `tmle` package, read the documentation, then call the `tmle` function using  Super Learner to estimate the conditional mean outcome $\mathrm{E}(Y|A,W)$ and  the exposure mechanism $\mathrm{P}(A|W)$. Use the `summary` function to obtain point estimates and get inference.**

**18. How do the estimates and confidence intervals from the TMLE function compare to your hand coded version with 1) the influence curve CIs and 2) the non-parametric bootstrap CIs?**
