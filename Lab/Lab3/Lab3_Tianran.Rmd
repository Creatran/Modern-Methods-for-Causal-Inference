---
title: "Lab3"
author: "Tianran Zhang"
date: "6/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(dplyr)
```

# Background Story üèπ

1. **Translate the DAG into the corresponding structural causal model.**

* Endogenous nodes: $X = (W_1, W_2, W_3, A, Y)$  
* Background (exogenous) variables: $U = (U_{W_1}, U_{W_2}, U_{W_3}, U_A, U_Y)$   
* Structural equations F:
  $$W_1 \leftarrow f_{W_1}(U_{W_1})$$ 
  $$W_2 \leftarrow f_{W_2}(U_{W_2})$$
  $$W_3 \leftarrow f_{W_3}(U_{W_3, A, Y})$$
  $$A \leftarrow f_A(U_A, W_1, W_2)$$
  $$Y \leftarrow f_Y(U_Y, A, W_1)$$


2. **Are there any exclusion restrictions or independence assumptions?**

There are exclusion restrictions: $W_1 \not\in P_a(W_2)$, $W_1, W_2 \not\in P_a(W_3)$,  $W_3 \not\in P_a(A)$, $W_2, W_3 \not\in P_a(Y)$.    
There is no independence assumptions.  

3. **Specify the causal question and parameter.**     
Causal question: Is there an effect of forming an alliance on the probability of surviving through the first 24 hours?    

The target causal parameter is the difference in the counterfactual probability of survival through the first 24 hours if an alliance is formed and the counterfactual probability of survival if an alliance is not formed.    
$$\theta^* = E^*[Y_1] - E^*[Y_0] = P^*(Y_1 = 1) - P^*(Y_0 = 1)$$


# Causal Parameters to Statistical Estimands

4. **Suppose the observed data consist of $n$ independent, identically distributed (i.i.d.) draws of the random variable  $O=(W_1, W_2, A, Y, W_3) \sim \mathrm{P}$. Explain the link between the SCM and the observed data. Does the structural causal model place any restrictions on the statistical model?**

We assume that the observed data were generated by a system compatible with (described by) our structural causal model. The distribution of the background variables $P^‚àó$ and the structural equations F identify the distribution of the observed data O. If we knew the distribution of U and the functions F, we would know the distribution of O.    
Thus, the causal model has implications for the distribution of O. These implications are testable.     

The SCM does not place any restrictions on the statistical model.     

5. **Explain the goal of identification. Briefly list and explain the three main assumptions we discussed in class to help us identify a causal parameter.**

The goal of identification is to estimate a property of the counterfactual‚Äôs distribution using only observed data. i.e., write $\theta^*$ as some $\theta$.      
Three main assumptions to help identify a causal parameter:   
Independence assumption, Randomization assumption, back-door assumption.    
If the unmeasured factors contributing to the outcome are independent of the unmeasured factors contributing to the exposure, i.e., $U_Y \perp \!\!\! \perp U_A$. The randomization assumption holds $Y_a \perp \!\!\! \perp A$. Back-door assumption and randomization assumption are both satisfied, the causal parameter is identified.     
    

6. **Assess identifiability of your causal parameter $\theta^*$. If not identified, under what assumptions would it be?**  

There is not any subset of observed covariates that satisÔ¨Åes the back-door criterion since there might be some unmeasured common causes among $U_A,U_{W_1},U{W_2}, U_{W_3}, U_Y$.   

Under the assumption that $U_Y \perp \!\!\! \perp U_A$, $\{W_1, W_2\}$ would satisfy the back-door criterion with respect to the effect of A on Y since a). Neither w1 nor w2 is a descendant of A; b). W1 and W2 blocks all ‚Äúback-door‚Äù paths from A to Y under the assumption that $U_Y \perp \!\!\! \perp U_A$.

7. **Write out the target statistical estimand in terms of the observed data distribution $\theta$.**

$$ \begin{align}
\theta &= E[E[Y|A = 1, W_1, W_2] - E[Y|A = 0, W_1, W_2]]\\
&= \sum_{w_1, w_2}[E[Y|A = 1, w_1, w_2] - E[Y|A = 0, w_1, w_2]]p(w_1, w_2)
\end{align}
$$

# A specific Data Generating Process (DGP)
8. **Evaluate $\theta$ in closed form for this particular DGP.**

```{r}
theta <- (plogis(-2+1 + 0.7 * 1)-plogis(-2+0+0.7*1))*0.45+
  (plogis(-2+1 + 0.7 * 0)-plogis(-2+0+0.7*0))*(1 - 0.45)
```

$$ \begin{align}
\theta &= E[E[Y|A = 1, W_1] - E[Y|A = 0, W_1]]\\
&= \sum_{w_1}[P[Y|A = 1, w_1] - E[P|A = 0, w_1]]p(w_1)\\
&= [P[Y|A = 1, W_1 = 1] - P[Y|A = 0, W_1 = 1]]p(W_1 = 1)  \\
&+ [P[Y|A = 1, W_1 = 0] - P[Y|A = 0, W_1 = 0]]p(W_1 = 0) \\
&= 0.177
\end{align}
$$

9. **Interpret $\theta$.**


10. **Translate the DGP into simulations.**
  a. **Set the seed to 343.**
  b. **Set the number of draws $n<-5000$.**
  c. **Sample $n$ i.i.d. observations of random variable $O=(W_1, W_2, A,Y, W_3) \sim \mathrm{P}$.**  In other words, simulate the background factors $U$ and evaluate the structural equations $F$. The $\mathrm{expit}$ function in `R` is `plogis`.
  d. **Create a data frame to hold these values.** The rows are the $n$ repetitions of the data generating process and the columns are the random variables. In other words, the rows are the $n$ subjects and the columns are their characteristics. **Use the `head` and `summary` functions to get a better understanding of the data generating process.**

```{r}
simulation <- function(n = 5000){
  UW1 <- runif(n)
  UW2 <- rnorm(n, 1, 2)
  UA  <- runif(n)
  UY <- runif(n)
  UW3 <- runif(n)

  W1 <- ifelse(UW1 < 0.45, 1, 0)
  W2 <- 0.75 * UW2
  A <- ifelse(UA < plogis(-1 + 2.6 * W1 + 0.9 * W2), 1, 0)
  Y <- ifelse(UY < plogis(-2 + A + 0.7 * W1), 1, 0)
  W3 <- ifelse(UW3 < plogis(-1 + 1.3 * A + 2.9 * Y), 1, 0)

  data.frame(W1, W2, W3, A, Y)
}

set.seed(343)
dat <- simulation()
head(dat)
summary(dat)

```



# Simple substitution estimator based on the G-Computation formula



## Implementation with the NPMLE

11. **Complete *Step 1* by estimating the conditional mean function with the non-parametric maximum likelihood estimator (NPMLE). Create strata of each possible value of $(A,W_1)$ and take the empirical mean of $Y$ in each strata.** This is equivalent to fitting a saturated regression model.
```{r}
meanY_a0w0 <- mean(dat$Y[dat$W1 == 0 & dat$A == 0])
meanY_a0w1 <- mean(dat$Y[dat$W1 == 1 & dat$A == 0])
meanY_a1w0 <- mean(dat$Y[dat$W1 == 0 & dat$A == 1])
meanY_a1w1 <- mean(dat$Y[dat$W1 == 1 & dat$A == 1])

data.frame(meanY_a0w0, meanY_a0w1, meanY_a1w0, meanY_a1w1) %>%
  kable()%>%
  kable_styling()
```

12. **Complete *Step 2* by estimating the marginal distribution $\mathrm{P}_0(W_1=w_1)$ with the sample proportion:**

```{r}
PW1 <- mean(dat$W1)
PW0 <- 1 - PW1
data.frame(PW1, PW0) 
```


13. **Substitute these estimates into the parameter mapping to complete *Step 3**.
```{r}
theta.hat <- (meanY_a1w1 - meanY_a0w1) * PW1 + (meanY_a1w0 - meanY_a0w0) * PW0
```

The estimated theta hat is `r round(theta.hat, 3)`.    


## Implementation with a "saturated" parametric regression

14. **Use the `glm` function to fit the conditional mean function $\mathrm{E}(Y|A,W_1)$ with logistic regression to complete *Step 1*.**

```{r}
fit1 <- glm(Y ~ A + W1, family = binomial, data = dat)

meanY <- predict(fit1, newdata = data.frame(A = c(1, 1, 0, 0), W1 = c(1, 0, 1, 0)), type = "response")

data.frame(A = c(1, 1, 0, 0), W1 = c(1, 0, 1, 0), meanY) %>%
  kable() %>%
  kable_styling()
```

15. **To implement *Step 2*:**
```{r}
strurated_regression <- function(dat){
  fit1 <- glm(Y ~ A + W1, family = binomial, data = dat)
  meanY <- predict(fit1, 
                   newdata = data.frame(A = c(1, 1, 0, 0), 
                                        W1 = c(1, 0, 1, 0)),
                   type = "response")

  trt <- control <- dat
  trt$A <- 1
  control$A <- 0
  
  E_A1W1 <- predict(fit1, newdata = trt, type = "response")
  E_A0W1 <- predict(fit1, newdata = control, type = "response")
  mean(E_A1W1 - E_A0W1)
}

```

16. **For *Step 3*, evaluate the statistical parameter by substituting the predicted mean outcomes under the treatment and under the control into the G-Computation formula.** 

```{r}
theta.hat2 <- strurated_regression(dat)
```

The statistical parameter estimated by this method is `r round(theta.hat2, 3)`.  

17. **Set `R` to 500 and `n` to 200, and then create a vector `estimates` of length $R=500$ to hold the estimated value of $\theta$ obtained  at each iteration.**

```{r}
r <- 500
n <- 200
estimates <- numeric(500)
```

18. **Inside a `for` loop from 1 to $R=500$, sample $n$ i.i.d. observations of random variable $O = (W_1,W_2,A,Y,W_3)$; implement the simple substitution estimator using the saturated regression model (adjusting for $A$, $W_1$ and their interaction), and save the resulting estimate $\hat{\theta}$ as an entry in the vector `estimates`.** 

```{r}
for (i in 1:r){
  dat1 <- simulation(n)
  estimates[i] <- strurated_regression(dat1)
}

```

19. **What is the average value of the estimates of $R=500$ simulations?**

The average value of the estimates of $R=500$ simulations is `r round(mean(estimates), 3)`.  

20. **Estimate the bias of the estimator. What is the average deviation of the estimate and the truth $\theta$?**  \[
Bias\big(\hat{\theta} \big) = \mathrm{E}( \hat{\theta} - \theta ) \]

```{r}
bias <- mean(estimates - theta)
```
The estimated bias of the estimator is `r round(bias, 4)`.


21. **Estimate the variance of the estimator.** How much do the estimates vary across samples?

```{r}
v <- mean((estimates - mean(estimates))^2)
```

The variance of the estimator is `r v` .   

25. **Estimate the mean squared error of the estimator.** On average, how far are the estimates from the truth?  
```{r}
mse <- mean((estimates - theta)^2)
```

The estimated mean squared error of the estimator is `r mse` .




